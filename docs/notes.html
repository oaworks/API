

Radical caching - cache is log is history (is test) is status is alerts is data
This approach needs some kind of name...

Cache everything - cache UI pages and static content of course, but also the result 
of every process that runs. We already do this to a high degree, but it was not 
foreseen as a core feature. And use remote workers to run cached processes where 
possible (e.g. with cloudflare workers)

If we cache everything, then it also makes sense to merge the concept of logging 
into caching - any process can log steps it takes, and the final cache result is 
the end of the log of that process. Again, we do this a lot, but it is not a core 
feature that is just available on every process as easily as a config option.
In some cases we store change history for certain processes on certain objects in 
certain collections. This is available as a config option of a collection, but we 
should make this a possible config of any process as well. So the cache is the log 
and it is also the change history.
If cache and log and history are combined, then that is all the data required for 
any status/alert system, so combine status/alerts into the core as well. Cache is 
the log and the history, which tells us the status and drives alerts.

For example 
at the moment we sometimes consider using Wikidata, which we had to locally cache 
because it does not offer a fast and powerful query language itself, but it also 
doesn’t offer an easy way to keep up to date with changes. So if we choose to 
continue using Wikidata, we need to develop a process to track changes and keep 
up to date with them. Similarly we also used a dump of Microsoft Academic Graph 
data, but have not yet developed a mechanism to keep track of changes in their 
dataset - although, in that case, we probably would decide not to keep using it 
once we do further analysis, because it is likely other sources give better coverage 
for less effort. OADOI on the other hand, is very useful, but performs fast enough 
that we probably wouldn’t gain much by caching it ourselves - however they do 
also offer a data dump we could use and track, if we wanted to do more complex 
queries than they offer on their API. These are the sorts of decisions we need 
to spend some time properly investigating though.

There's always a core of useful bits that are necessary for any system - auth (plus 
accounts and roles), it needs to be accessible somehow (via the API), to receive input and give output, it needs to store 
and retrieve data, conceptualise data in ways people want to use it, and it needs to log stuff 
that happens for later debugging, and also receive alerts from remote parts such 
as embeds on other websites, and show us status pages and send us alerts when necessary.

Then there are secondary parts. Some of what we used to consider secondary ended 
up being used a lot, so it became a hassle to use it repeatedly in ways that had 
not been considered core.

Then we have other things that we use remotely - systems 
run by other people that we want to query and get data from, or send data to. 

And finally we have the services - things we write that use the rest of Paradigm, 
and they provide a specific service over a defined set of API routes, such 
as Open Access Button. Technically there is also the "frontend" layer, but that 
is mostly outside Paradigm itself. There are also various third party services we use 
as part of our infrastructure, so I’ll list them for completeness as well.

Written in node.js, using coffeescript or javascript, and able to run other 
languages

Why coffeescript? We use coffeescript because it gave us advantages in simplicity of 
writing that were not present in js/node at the time. Since then, js/node have 
incorporated quite a few of the things that made coffeescript famous, so it is 
less beneficial now. From this year, the coffeescript developers have decided 
not to keep updating it, so over time it is going to become obsolete compared 
to advancing node features. I considered getting rid of coffeescript altogether 
because of this, but coffeescript is still a little bit nicer syntactically, 
so I’ll keep using it. It was very popular seven years ago, and for a few years 
since, but over the last year or two is definitely becoming less so because there 
is not much reason for newer people to learn it now that node or js itself is 
better for the sync/async stuff. Coffeescript is just a pseudo-language that 
“compiles” down to javascript, so the code that eventually runs (either in our 
current system or on CF workers) is already javascript anyway. Also, we can 
already directly write EITHER coffeescript or javascript in the code repo itself, 
and they work seamlessly side by side. I’ve also already tested if it complies and 
deploys successfully into CF workers, and it does. So moving away from coffeescript 
at any time in future is no harder than it is right now, and all it requires is that 
we take the code we already generate for deployment, and start writing directly into 
that in future, instead of the coffeescript. This is very easy - like, a few minutes 
work. So I think for now it’s worth keeping coffeescript because of some nice little 
benefits for me personally, because the trade-off is virtually no cost to the rest 
of the project.

What about Typescript? It doesn’t offer any benefits that we need, 
but is similar conceptually to coffeescript. Someone could write typescript in our 
API already, as it also ends up being javascript anyway. Some of the main benefits 
of ts have also now become part of modern js, and really the only big remaining 
benefit of ts, I think, is being able to set and control the “type” of a variable. 
We don’t need that sort of control, 
and actually I’d say it would be an unnecessary limitation for us to have it. Some 
people who really like that would prefer it though. I would only use that when writing 
code that really needed it, and in those cases I would most likely not be writing in 
any kind of javascript anyway. The good thing about this though, as with coffeescript, 
is that we don’t gain or lose by not using it now - it all ends up as js anyway, 
so we’re open to it at any time.

Paradigm places no constraints on what kind of UI it requires - it's just an API, 
and anything can connect to it. There are some useful client-side javascript 
libraries that are helpful on 
browser UIs, such as one that interacts with the Paradigm login API 
(login/auth is pretty complex so this is the most useful thing to provide to the frontend). 
Paradigm is intentionally designed this way so that frontends can be kept simple, 
and can be nothing more than some js libraries and static sites, which are very 
easy to deploy and extremely stable. I am aware of many more complex frontend 
frameworks and integrated frameworks, but none have ever provided sufficient 
functionality to be worth the extra complexity they bring. So we keep it as 
simple as possible, and provide as many useful features to connect to via the 
API as we can. It also means it is easier to create a new frontend / UI, such 
as a browser plugin or an android app, because it just has to connect to the 
necessary route to get the answer to the complicated parts. This sometimes 
takes a bit of unorthodox thinking when it comes to how to develop a frontend / UI, 
but it is worth it, and we are not doing something weird all on our own - 
Google takes this approach too, and for example we’ve been well ahead of the 
curve on things like passwordless login from static pages by a few years.




======================================================================

Some useful stuff from installs, make better instructions out of this:

google chrome still requires manual install as well to use puppeteer
the default node install of puppeteer still failed to find its own install of chromium
see the puppet file for info on installing chrome

# install pm2, start the server, add to startup script, and save
# start with watch to restart on changes, and -i with a number to set how many instances to start (should be less than number of CPU available)
# and --name to name them, so can have easy names for dev and live for example
# https://pm2.keymetrics.io/docs/usage/cluster-mode/
npm i -g pm2
pm2 install pm2-logrotate
pm2 start server/dist/server.min.js -i 3 --name paradigm --watch ( --node-args="--max-old-space-size=3072" )
pm2 startup
pm2 save
pm2 logs # will show log output and console.log
pm2 monit # a terminal monitor ui - note that pm2 logs can be easier for quickly following incoming requests, but monit gives some more overview stats
# logs default get written to ~/.pm2/logs





